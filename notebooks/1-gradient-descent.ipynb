{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/_static/logo-rsm.png\" align=\"right\" width=\"200\">\n",
    "\n",
    "\n",
    "# Part 1: Gradient Descent\n",
    "\n",
    "This notebook studies inference for a ___linear model___ using gradient descent.\n",
    "\n",
    "Our focus will be using `pytorch` to make the implementation more parsimonious:\n",
    "1. Generate data for a linear model\n",
    "1. Review gradient descent, implement inference in `numpy` (including coding the gradients)\n",
    "1. Implement inference using PyTorch's autograd ðŸŽ‰ \n",
    "1. Leverage `torch.nn` module to simplify the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.common import print_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Data\n",
    "\n",
    "Our data generating process is a linear model\n",
    "\n",
    "$\n",
    "\\quad \\mathbf y = a + b\\, \\mathbf x_1 + c\\, \\mathbf x_2 + d\\, \\mathbf x_3 + \\mathbf\\varepsilon\n",
    "$\n",
    "\n",
    "We'll set\n",
    "\n",
    "- $a = 3$\n",
    "- $b = -2$\n",
    "- $c = 1$\n",
    "- $d = -1$\n",
    "- $\\varepsilon \\sim \\mathcal{N}(\\mu_{\\varepsilon}, \\sigma_{\\varepsilon})$ with $\\mu_{\\varepsilon}=0$ and $\\sigma_{\\varepsilon}=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# data size\n",
    "N = 1_000\n",
    "\n",
    "# parameters\n",
    "at = 3\n",
    "bt = -2\n",
    "ct = 1\n",
    "dt = -1\n",
    "sigma_err = 1\n",
    "\n",
    "# generate data\n",
    "err = np.random.normal(0, sigma_err, N)\n",
    "x1 = np.random.normal(2, 1, N)\n",
    "x2 = np.random.normal(3, 1, N)\n",
    "x3 = np.random.normal(1, 1, N)\n",
    "y = at + bt * x1 + ct * x2 + dt * x3 + err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Manual implementation with `numpy`\n",
    "\n",
    "1. Manually implement loss function $\\mathcal{L} = \\frac 1 2 (\\hat y - y)^2$\n",
    "1. Manually calculate gradient $\\partial_a \\mathcal{L} = \\hat y - y$\n",
    "1. Manually update parameters $\\quad a^{i+1} = a^{i} - \\alpha \\partial_a \\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.586737\n",
      "   19999 | loss = 0.504840\n",
      "   29999 | loss = 0.481684\n",
      "   39999 | loss = 0.475138\n",
      "   49999 | loss = 0.473287\n",
      "   59999 | loss = 0.472763\n",
      "   69999 | loss = 0.472615\n",
      "   79999 | loss = 0.472574\n",
      "   89999 | loss = 0.472562\n",
      "   99999 | loss = 0.472558\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 2.90 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n"
     ]
    }
   ],
   "source": [
    "# define learning rate\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "# random starting values\n",
    "am, bm, cm, dm = np.random.uniform(-1, 1, 4)\n",
    "\n",
    "# training: looping over epochs\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    y_pred = am + bm * x1 + cm * x2 + dm * x3\n",
    "\n",
    "    loss = np.mean((y_pred - y) ** 2 / 2)\n",
    "\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss:.6f}\")\n",
    "\n",
    "    grad_y_pred = y_pred - y\n",
    "\n",
    "    grad_a = (grad_y_pred * 1).mean()\n",
    "    grad_b = (grad_y_pred * x1).mean()\n",
    "    grad_c = (grad_y_pred * x2).mean()\n",
    "    grad_d = (grad_y_pred * x3).mean()\n",
    "\n",
    "    am -= learning_rate * grad_a  # am = am - learning_rate * grad_a\n",
    "    bm -= learning_rate * grad_b\n",
    "    cm -= learning_rate * grad_c\n",
    "    dm -= learning_rate * grad_d\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Use `autograd` from `PyTorch`\n",
    "\n",
    "1. Use `torch` gradients\n",
    "1. Manually update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some first steps\n",
    "\n",
    "#### Turn `numpy` arrays into `torch` tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1t = torch.tensor(x1)\n",
    "x2t = torch.tensor(x2)\n",
    "x3t = torch.tensor(x3)\n",
    "yt = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((), requires_grad=True)\n",
    "b = torch.randn((), requires_grad=True)\n",
    "c = torch.randn((), requires_grad=True)\n",
    "d = torch.randn((), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4373, -4.2672, -2.1062, -2.8174, -3.1186, -3.7332, -2.9870, -3.0341,\n",
       "        -0.9524, -1.0748], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_pred = a + b * x1t + c * x2t + d * x3t\n",
    "yt_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3387, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.996028184890747, -2.996028283779662)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.item(), (yt_pred - yt).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.8542213439941406, -3.854221284804097)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad.item(), ((yt_pred - yt) * x1t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.999500274658203, -10.999500625744313)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad.item(), ((yt_pred - yt) * x2t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.729453444480896, -1.7294534728842264)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad.item(), ((yt_pred - yt) * x3t).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.499813\n",
      "   19999 | loss = 0.480265\n",
      "   29999 | loss = 0.474737\n",
      "   39999 | loss = 0.473174\n",
      "   49999 | loss = 0.472732\n",
      "   59999 | loss = 0.472607\n",
      "   69999 | loss = 0.472571\n",
      "   79999 | loss = 0.472561\n",
      "   89999 | loss = 0.472558\n",
      "   99999 | loss = 0.472557\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 2.90 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch:  y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "ap = torch.randn((), requires_grad=True)\n",
    "bp = torch.randn((), requires_grad=True)\n",
    "cp = torch.randn((), requires_grad=True)\n",
    "dp = torch.randn((), requires_grad=True)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = ap + bp * x1t + cp * x2t + dp * x3t\n",
    "    loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ap -= learning_rate * ap.grad\n",
    "        bp -= learning_rate * bp.grad\n",
    "        cp -= learning_rate * cp.grad\n",
    "        dp -= learning_rate * dp.grad\n",
    "\n",
    "        ap.grad = None\n",
    "        bp.grad = None\n",
    "        cp.grad = None\n",
    "        dp.grad = None\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Using `torch.nn`\n",
    "\n",
    "1. Use `torch` loss function\n",
    "1. Use `torch` gradients\n",
    "1. Use `torch` parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor(np.column_stack([x1, x2, x3]).astype(np.float32))\n",
    "yt = torch.tensor(y.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.519029\n",
      "   19999 | loss = 0.485698\n",
      "   29999 | loss = 0.476274\n",
      "   39999 | loss = 0.473609\n",
      "   49999 | loss = 0.472855\n",
      "   59999 | loss = 0.472641\n",
      "   69999 | loss = 0.472581\n",
      "   79999 | loss = 0.472564\n",
      "   89999 | loss = 0.472559\n",
      "   99999 | loss = 0.472558\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 2.90 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch:  y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch2: y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = model(xt)\n",
    "    loss = loss_fn(yt_pred, yt) / 2\n",
    "\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # udpate\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")\n",
    "print_result(torch.cat([model[0].bias, model[0].weight.flatten()]), \"PyTorch2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "&mdash; <br>\n",
    "Sebastian Gabel <br>\n",
    "Rotterdam School of Management <br>\n",
    "\n",
    "www.github.com/sbstn-gbl <br>\n",
    "www.sebastiangabel.com <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Inspired by this [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some first steps\n",
    "\n",
    "#### Turn `numpy` arrays into `torch` tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1t = torch.tensor(x1)\n",
    "x2t = torch.tensor(x2)\n",
    "x3t = torch.tensor(x3)\n",
    "yt = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((), requires_grad=True)\n",
    "b = torch.randn((), requires_grad=True)\n",
    "c = torch.randn((), requires_grad=True)\n",
    "d = torch.randn((), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5723, 2.4670, 2.9830, 2.0257, 6.6208, 2.1603, 6.0776, 4.6468, 2.9855,\n",
       "        2.5320], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_pred = a + b * x1t + c * x2t + d * x3t\n",
    "yt_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8533, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4855661392211914, 2.4855661035428267)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.item(), (yt_pred - yt).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.250786781311035, 8.250786465544447)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad.item(), ((yt_pred - yt) * x1t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.396797180175781, 7.3967969924699934)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad.item(), ((yt_pred - yt) * x2t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.93881893157959, 3.938818870680674)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad.item(), ((yt_pred - yt) * x3t).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.499813\n",
      "   19999 | loss = 0.480265\n",
      "   29999 | loss = 0.474737\n",
      "   39999 | loss = 0.473174\n",
      "   49999 | loss = 0.472732\n",
      "   59999 | loss = 0.472607\n",
      "   69999 | loss = 0.472571\n",
      "   79999 | loss = 0.472561\n",
      "   89999 | loss = 0.472558\n",
      "   99999 | loss = 0.472557\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 2.90 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch:  y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "ap = torch.randn((), requires_grad=True)\n",
    "bp = torch.randn((), requires_grad=True)\n",
    "cp = torch.randn((), requires_grad=True)\n",
    "dp = torch.randn((), requires_grad=True)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = ap + bp * x1t + cp * x2t + dp * x3t\n",
    "    loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ap -= learning_rate * ap.grad\n",
    "        bp -= learning_rate * bp.grad\n",
    "        cp -= learning_rate * cp.grad\n",
    "        dp -= learning_rate * dp.grad\n",
    "\n",
    "        ap.grad = None\n",
    "        bp.grad = None\n",
    "        cp.grad = None\n",
    "        dp.grad = None\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor(np.column_stack([x1, x2, x3]).astype(np.float32))\n",
    "yt = torch.tensor(y.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.519029\n",
      "   19999 | loss = 0.485698\n",
      "   29999 | loss = 0.476274\n",
      "   39999 | loss = 0.473609\n",
      "   49999 | loss = 0.472855\n",
      "   59999 | loss = 0.472641\n",
      "   69999 | loss = 0.472581\n",
      "   79999 | loss = 0.472564\n",
      "   89999 | loss = 0.472559\n",
      "   99999 | loss = 0.472558\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 2.90 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch:  y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n",
      "PyTorch2: y = 2.91 + -1.99 x_1 + 1.04 x_2 + -1.02 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = model(xt)\n",
    "    loss = loss_fn(yt_pred, yt) / 2\n",
    "\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # udpate\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")\n",
    "print_result(torch.cat([model[0].bias, model[0].weight.flatten()]), \"PyTorch2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "&mdash; <br>\n",
    "Sebastian Gabel <br>\n",
    "Rotterdam School of Management <br>\n",
    "\n",
    "www.github.com/sbstn-gbl <br>\n",
    "www.sebastiangabel.com <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Inspired by this [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
